---
layout: page
title: Wenhao (Reself) Chai
subtitle: master @UW | research intern @MSRA
use-site-title: false
---

<head>
	<script src="https://kit.fontawesome.com/5bef57b3e9.js" crossorigin="anonymous"></script>
</head>

<br>
Wenhao Chai is a master student at University of Washington, with <a href="https://ipl-uw.github.io/">Information Processing Lab</a> advised by Prof. <a href="https://people.ece.uw.edu/hwang/">Jenq-Neng Hwang</a>. Previously, he was an undergradate student at Zhejiang University, advised by Prof. <a href="https://person.zju.edu.cn/en/gaoangwang/">Gaoang Wang</a>. He is fortunate to have internship at Media Computing Group, Microsoft Research Asia, advised by Dr. <a href="https://scholar.google.com/citations?user=Ow4R8-EAAAAJ&hl=en&oi=ao">Xun Guo</a>. His research interests include human pose estimation, diffusion model, video understanding, and multi-modality learning.
<br>
<br>
I am holding a team to collaborate with <a href="https://github.com/open-mmlab">OpenMMLab</a>. Our goal is to build a <b>Universal Human Model of Perception, Understanding, and Planning in the Sports Scene</b>. We have 5 Ph.D. students and 5 undergradutes involved currently. Contact me (wchai@uw.edu) if you are interested.
<br>
<hr style="height:2px;border-width:0;color:gray;background-color:gray">
<b><i class="fa-solid fa-pen-to-square" style="font-size:24px"></i> Updates:</b><br><br>

<ul>
	<li><i>June 2023:</i> <img src="static/imgs/zju.png" width="25" height="25" style="vertical-align:text-bottom"/> I graduate from Zhejiang University.
	</li><br>
	
	<li><i>May 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Five A+ Network: You Only Need 9K Parameters for Underwater Image Enhancement</i> released at <a href="https://arxiv.org/abs/2305.08824">arXiv</a> with <a href="https://github.com/Owen718/FiveAPlus-Network">codes</a>. We propose a highly efficient and lightweight real-time underwater image enhancement network with only ∼ 9k parameters and ∼ 10ms processing time.
	</li><br>
	
	<li><i>Apr 2023: <img src="static/imgs/microsoft.png" width="25" height="25" style="vertical-align:text-bottom"/> Start on-site internship at Microsoft Research Asia (MSRA), Beijing.
	</li><br>

	<li><i>Apr 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> The short version of our paper <i>DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models</i> is accepted by CVPR 2023 <a href="http://conferences.visionbib.com/2023/cvpr-cvfad-6-23-call.html">6th Workshop on Computer Vision for Fashion, Art, and Design</a>.
	</li><br>

	<li><i>Mar 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation</i> released at <a href="https://arxiv.org/abs/2303.16456">arXiv</a>.
	</li><br>

	<li><i>Mar 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Blind Inpainting with Object-aware Discrimination for Artificial Marker Removal</i> released at <a href="https://arxiv.org/abs/2303.15124">arXiv</a>.
	</li><br>

	<li><i>Mar 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Deep Learning Methods for Small Molecule Drug Discovery: A Survey</i> is accepted at IEEE Transactions on Artificial Intelligence.
	</li><br>

	<li><i>Mar 2023:</i> <img src="static/imgs/uw.png" width="36" height="25" style="vertical-align:text-bottom"/> Become a graduate student member of <a href="https://ipl-uw.github.io/">Information Processing Lab</a> at University of Washington, advised by Professor <a href="https://people.ece.uw.edu/hwang/">Jenq-Neng Hwang</a>.
	</li><br>

	<li><i>Feb 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models</i> released at <a href="https://arxiv.org/abs/2302.06826">arXiv</a> with <a href="https://github.com/Rem105-210/DiffFashion">codes</a>.
	</li><br>
	
	<li><i>Feb 2023:</i> <img src="static/imgs/microsoft.png" width="25" height="25" style="vertical-align:text-bottom"/> Become a research intern at <a href="https://www.msra.cn/">Microsoft Research Asia (MSRA)</a>, advised by principal researcher <a href="https://scholar.google.com/citations?user=Ow4R8-EAAAAJ&hl=en&oi=ao">Xun Guo</a>.
	</li><br>

	<li><i>Oct 2022:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Automatic Spinal Ultrasound Image Segmentation and Deployment for Real-time Spine Volumetric Reconstruction</i> accepted at <a href="https://icus2022.c2.org.cn/">ICUS 2022</a> with best paper award.
	</li><br>

	<li><i>Sep 2022:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Weakly Supervised Two-Stage Training Scheme for Deep Video Fight Detection Model</i> accepted at <a href="https://ictai.computer.org/2022/">ICTAI 2022</a>. we release a new dataset, <a href="https://github.com/rese1f/VideoFightDetection">VFD-2000</a>, that specializes in video fight detection, with a larger scale and more scenarios than existing datasets.
	</li><br>

	<li><i>Aug 2022:</i> <img src="static/imgs/alibaba.png" width="36" height="25" style="vertical-align:text-bottom"/> Visit <a href="https://damo.alibaba.com">DAMO Academy</a> at Alibaba.
	</li><br>

	<li><i>July 2022:</i> <img src="static/imgs/uiuc.png" width="18" height="25" style="vertical-align:text-bottom"/> Become a member of <a href="https://www.ncsa.illinois.edu/">National Center for Supercomputing Applications (NCSA)</a> at <a href="https://illinois.edu/">University of Illinois Urbana-Champaign</a>, work with Professor <a href="https://cs.illinois.edu/about/people/faculty/kindrtnk">Volodymyr (Vlad) Kindratenko</a>.
	</li><br>

	<li><i>June 2022:</i> Attend <a href="http://cvpr2022.thecvf.com/">CVPR 2022</a> in New Orleans and made a lot of friends there. Here are my <a href="https://github.com/rese1f/awesome-cvpr2022">notes</a>.
	</li><br>

	<li><i>June 2022:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Deep Vision Multimoda!l Learning: Methodology, Benchmark, and Trend</i> accepted at Applied Science (Q2).
	</li><br>

	<li><i>July 2021:</i> Start my research on 3D human pose estimation task, advised by Professor <a href="https://person.zju.edu.cn/en/gaoangwang/">Gaoang Wang</a>.
	</li><br>

</ul>
