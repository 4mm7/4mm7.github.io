---
layout: page
title: Wenhao (Reself) Chai
subtitle: undergrad @ZJU | master @UW | research intern @MSRA
use-site-title: true
---

<head>
	<script src="https://kit.fontawesome.com/5bef57b3e9.js" crossorigin="anonymous"></script>
</head>

<br>
	I am an undergradate student at <a href="https://www.zju.edu.cn/">Zhejiang University</a>, advised by <a href="https://person.zju.edu.cn/en/gaoangwang/">Gaoang Wang</a>. My research interests lie in 3D human pose estimation and multi-modality learning. I am currently working on <b>diffusion model</b> and <b>multi-modality learning</b> for generative tasks.
<br>

<br>
	When I am not doing research, I like ðŸ“·photography, ðŸš—traveling, and ðŸŽ¤singing.
<br>

<hr style="height:2px;border-width:0;color:gray;background-color:gray">

<b><i class="fa-solid fa-pen-to-square" style="font-size:24px"></i> Updates:</b><br><br>

<ul>
	<li><i>Apr 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> The short version of our paper <i>DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models</i> is accepted by CVPR 2023 <a href="http://conferences.visionbib.com/2023/cvpr-cvfad-6-23-call.html">6th Workshop on Computer Vision for Fashion, Art, and Design</a>!
	</li><br>

	<li><i>Mar 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation</i> released at <a href="https://arxiv.org/abs/2303.16456">arXiv</a>!
	</li><br>

	<li><i>Mar 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Blind Inpainting with Object-aware Discrimination for Artificial Marker Removal</i> released at <a href="https://arxiv.org/abs/2303.15124">arXiv</a>!
	</li><br>

	<li><i>Mar 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Deep Learning Methods for Small Molecule Drug Discovery: A Survey</i> is accepted at IEEE Transactions on Artificial Intelligence.
	</li><br>

	<li><i>Mar 2023:</i> Become a graduate student member of <a href="https://ipl-uw.github.io/">Information Processing Lab</a> at University of Washington, advised by Professor <a href="https://people.ece.uw.edu/hwang/">Jenq-Neng Hwang</a>.
	</li><br>

	<li><i>Feb 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models</i> released at <a href="https://arxiv.org/abs/2302.06826">arXiv</a> with <a href="https://github.com/Rem105-210/DiffFashion">codes</a>!
	</li><br>
	
	<li><i>Feb 2023:</i> <img src="static/imgs/microsoft.png" width="25" height="25" style="vertical-align:text-bottom"/> Become a research intern at <a href="https://www.msra.cn/">Microsoft Research Asia (MSRA)</a>, advised by principal researcher <a href="https://scholar.google.com/citations?user=Ow4R8-EAAAAJ&hl=en&oi=ao">Xun Guo</a>.
	</li><br>

	<li><i>Oct 2022:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Automatic Spinal Ultrasound Image Segmentation and Deployment for Real-time Spine Volumetric Reconstruction</i> accepted at <a href="https://icus2022.c2.org.cn/">ICUS 2022</a> with best paper award!
	</li><br>

	<li><i>Sep 2022:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Weakly Supervised Two-Stage Training Scheme for Deep Video Fight Detection Model</i> accepted at <a href="https://ictai.computer.org/2022/">ICTAI 2022</a>. we release a new dataset, <a href="https://github.com/rese1f/VideoFightDetection">VFD-2000</a>, that specializes in video fight detection, with a larger scale and more scenarios than existing datasets.
	</li><br>

	<li><i>Aug 2022:</i> <img src="static/imgs/alibaba.png" width="36" height="25" style="vertical-align:text-bottom"/> Visit <a href="https://damo.alibaba.com">DAMO Academy</a> at Alibaba.
	</li><br>

	<li><i>July 2022:</i> <img src="static/imgs/uiuc.png" width="18" height="25" style="vertical-align:text-bottom"/> Become a member of <a href="https://www.ncsa.illinois.edu/">National Center for Supercomputing Applications (NCSA)</a> at <a href="https://illinois.edu/">University of Illinois Urbana-Champaign</a>, work with Professor <a href="https://cs.illinois.edu/about/people/faculty/kindrtnk">Volodymyr (Vlad) Kindratenko</a>.
	</li><br>

	<li><i>June 2022:</i> Attend <a href="http://cvpr2022.thecvf.com/">CVPR 2022</a> in New Orleans and made a lot of friends there. Here are my <a href="https://github.com/rese1f/awesome-cvpr2022">notes</a>.
	</li><br>

	<li><i>June 2022:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Deep Vision Multimodal Learning: Methodology, Benchmark, and Trend</i> accepted at Applied Science (Q2).
	</li><br>

	<li><i>July 2021:</i> Start my research on 3D human pose estimation task, advised by Professor <a href="https://person.zju.edu.cn/en/gaoangwang/">Gaoang Wang</a>.
	</li><br>

</ul>